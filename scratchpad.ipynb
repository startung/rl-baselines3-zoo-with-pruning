{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import difflib\n",
    "import importlib\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import stable_baselines3 as sb3\n",
    "import torch as th\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "# Register custom envs\n",
    "# import rl_zoo3.import_envs  # noqa: F401\n",
    "from rl_zoo3.exp_manager import ExperimentManager\n",
    "# from rl_zoo3.utils import ALGOS, StoreDict\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RlZooArgs:\n",
    "    algo: str = 'ppo' # RL Algorithm\n",
    "    env: str = 'CartPole-v1' # environment ID\n",
    "    tensorboard_log: str = '' # Tensorboard log dir\n",
    "    trained_agent: str = '' # Path to a pretrained agent to continue training\n",
    "    truncate_last_trajectory: bool = True # When using HER with online sampling the last trajectory in the replay buffer will be truncated after reloading the replay buffer.\n",
    "    n_timesteps: int = -1 # Overwrite the number of timesteps\n",
    "    num_threads: int = -1 # Number of threads for PyTorch (-1 to use default)\n",
    "    log_interval: int = -1 # Override log interval ( default: -1, no change)\n",
    "    eval_freq: int = 25000 # Evaluate the agent every n steps (if negative, no evaluation). During hyperparameter optimization n-evaluations is used instead\n",
    "    optimization_log_path: str = '' # Path to save the evaluation log and optimal policy for each hyperparameter tried during optimization. Disabled if no argument is passed.\n",
    "    eval_episodes: int = 5 # Number of episodes to use for evaluation\n",
    "    n_eval_envs: int = 1 # Number of environments for evaluation\n",
    "    save_freq: int = -1 # Save the model every n steps (if negative, no checkpoint)\n",
    "    save_replay_buffer: bool = False # Save the replay buffer too (when applicable)\n",
    "    log_folder: str = 'logs' # Log folder\n",
    "    seed: int = 44113 # -1 # Random generator seed\n",
    "    vec_env: str = 'dummy' # VecEnv type\n",
    "    device: str = 'auto' # PyTorch device to be use (ex: cpu, cuda...)\n",
    "    n_trials: int = 500 # Number of trials for optimizing hyperparameters. This applies to each optimization runner, not the entire optimization process.\n",
    "    max_total_trials: int = 0 # Number of (potentially pruned) trials for optimizing hyperparameters. This applies to the entire optimization process and takes precedence over --n-trials if set.\n",
    "    optimize_hyperparameters: bool = False # Run hyperparameters search\n",
    "    no_optim_plots: bool = False # Disable hyperparameter optimization plots\n",
    "    n_jobs: int = 1 # Number of parallel jobs when optimizing hyperparameters\n",
    "    sampler: str = 'tpe' # Sampler to use when optimizing hyperparameters\n",
    "    pruner: str = 'median' # Pruner to use when optimizing hyperparameters\n",
    "    n_startup_trials: int = 10 # Number of trials before using optuna sampler\n",
    "    n_evaluations: int = 0 # Training policies are evaluated every n-timesteps // n_evaluations steps when doing hyperparameter optimization.Default is 1 evaluation per 100k timesteps.\n",
    "    storage: str = '' # Database storage path if distributed optimization should be used\n",
    "    study_name: str = '' # Study name for distributed optimization\n",
    "    verbose: int = 0 # Verbose mode (0: no output, 1: INFO)\n",
    "    gym_packages: str = '' # Additional external Gym environment package modules to import\n",
    "    env_kwargs: str = '' # Optional keyword argument to pass to the env constructor\n",
    "    eval_env_kwargs: str = '' # Optional keyword argument to pass to the env constructor for evaluation\n",
    "    hyperparams: str = '' # Overwrite hyperparameter (e.g. learning_rate:0.01 train_freq:10)\n",
    "    conf_file: str = '' # Custom yaml file or python package from which the hyperparameters will be loaded.We expect that python packages contain a dictionary called 'hyperparams' which contains a key for each environment.\n",
    "    uuid: bool = False # Ensure that the run has a unique ID\n",
    "    track: bool = False # if toggled, this experiment will be tracked with Weights and Biases\n",
    "    wandb_project_name: str = 'sb3' # the wandb's project name\n",
    "    wandb_entity: str = '' # the entity (team) of wandb's project\n",
    "    progress: bool = False # if toggled, display a progress bar using tqdm and rich\n",
    "    wandb_tags: str = '' # Tags for wandb run, e.g.: -tags optimized pr-123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  RL Algo |  BipedalWalker-v3 | LunarLander-v2 | LunarLanderContinuous-v2 |  BipedalWalkerHardcore-v3 | CarRacing-v0 |\n",
    "|----------|--------------|----------------|------------|--------------|--------------------------|\n",
    "| ARS      |  | :heavy_check_mark: | | :heavy_check_mark: | |\n",
    "| A2C      | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | |\n",
    "| PPO      | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | |\n",
    "| DQN      | N/A | :heavy_check_mark: | N/A | N/A | N/A |\n",
    "| QR-DQN   | N/A | :heavy_check_mark: | N/A | N/A | N/A |\n",
    "| DDPG     | :heavy_check_mark: | N/A | :heavy_check_mark: | | |\n",
    "| SAC      | :heavy_check_mark: | N/A | :heavy_check_mark: | :heavy_check_mark: | |\n",
    "| TD3      | :heavy_check_mark: | N/A | :heavy_check_mark: | :heavy_check_mark: | |\n",
    "| TQC      | :heavy_check_mark: | N/A | :heavy_check_mark: | :heavy_check_mark: | |\n",
    "| TRPO     | | :heavy_check_mark: | :heavy_check_mark: | | |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "    \"ars\": ['CartPole-v1', 'MountainCar-v0', 'Acrobot-v1', 'Pendulum-v1', 'MountainCarContinuous-v0'],\n",
    "    \"a2c\": ['CartPole-v1', 'MountainCar-v0', 'Acrobot-v1', 'Pendulum-v1', 'MountainCarContinuous-v0'],\n",
    "    \"ppo\": ['CartPole-v1', 'MountainCar-v0', 'Acrobot-v1', 'Pendulum-v1', 'MountainCarContinuous-v0'],\n",
    "    \"dqn\": ['CartPole-v1', 'MountainCar-v0', 'Acrobot-v1'],\n",
    "    \"qrdqn\": ['CartPole-v1', 'MountainCar-v0', 'Acrobot-v1'],\n",
    "    \"ddpg\": ['Pendulum-v1', 'MountainCarContinuous-v0'],\n",
    "    \"sac\": ['Pendulum-v1', 'MountainCarContinuous-v0'],\n",
    "    \"td3\": ['Pendulum-v1', 'MountainCarContinuous-v0'],\n",
    "    \"tqc\": ['Pendulum-v1', 'MountainCarContinuous-v0'],\n",
    "    \"trpo\": ['CartPole-v1', 'MountainCar-v0', 'Acrobot-v1', 'Pendulum-v1', 'MountainCarContinuous-v0'],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ars on CartPole-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ars.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('n_delta', 2),\n",
      "             ('n_envs', 1),\n",
      "             ('n_timesteps', 50000.0),\n",
      "             ('policy', 'LinearPolicy')])\n",
      "Log path: logs/ars/CartPole-v1_4\n",
      "policy:\n",
      "Running ars on MountainCar-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ars.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('delta_std', 0.1),\n",
      "             ('learning_rate', 0.018),\n",
      "             ('n_delta', 8),\n",
      "             ('n_envs', 1),\n",
      "             ('n_timesteps', 500000.0),\n",
      "             ('n_top', 1),\n",
      "             ('normalize', 'dict(norm_obs=True, norm_reward=False)'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[16])'),\n",
      "             ('zero_policy', False)])\n",
      "Log path: logs/ars/MountainCar-v0_4\n",
      "policy:\n",
      "Running ars on Acrobot-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ars.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('delta_std', 0.1),\n",
      "             ('learning_rate', 0.018),\n",
      "             ('n_delta', 4),\n",
      "             ('n_envs', 1),\n",
      "             ('n_timesteps', 500000.0),\n",
      "             ('n_top', 1),\n",
      "             ('normalize', 'dict(norm_obs=True, norm_reward=False)'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[16])'),\n",
      "             ('zero_policy', False)])\n",
      "Log path: logs/ars/Acrobot-v1_4\n",
      "policy:\n",
      "Running ars on Pendulum-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ars.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('delta_std', 0.1),\n",
      "             ('learning_rate', 0.018),\n",
      "             ('n_delta', 4),\n",
      "             ('n_envs', 1),\n",
      "             ('n_timesteps', 2000000.0),\n",
      "             ('n_top', 1),\n",
      "             ('normalize', 'dict(norm_obs=True, norm_reward=False)'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[16])'),\n",
      "             ('zero_policy', False)])\n",
      "Log path: logs/ars/Pendulum-v1_3\n",
      "policy:\n",
      "Running ars on MountainCarContinuous-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ars.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('delta_std', 0.2),\n",
      "             ('learning_rate', 0.018),\n",
      "             ('n_delta', 4),\n",
      "             ('n_envs', 1),\n",
      "             ('n_timesteps', 500000.0),\n",
      "             ('n_top', 1),\n",
      "             ('normalize', 'dict(norm_obs=True, norm_reward=False)'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[16])'),\n",
      "             ('zero_policy', False)])\n",
      "Log path: logs/ars/MountainCarContinuous-v0_3\n",
      "policy:\n",
      "Running a2c on CartPole-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/a2c.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.0),\n",
      "             ('n_envs', 8),\n",
      "             ('n_timesteps', 500000.0),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/a2c/CartPole-v1_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running a2c on MountainCar-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/a2c.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.0),\n",
      "             ('n_envs', 16),\n",
      "             ('n_timesteps', 1000000.0),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/a2c/MountainCar-v0_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running a2c on Acrobot-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/a2c.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.0),\n",
      "             ('n_envs', 16),\n",
      "             ('n_timesteps', 500000.0),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/a2c/Acrobot-v1_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running a2c on Pendulum-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/a2c.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.9),\n",
      "             ('gamma', 0.9),\n",
      "             ('learning_rate', 'lin_7e-4'),\n",
      "             ('max_grad_norm', 0.5),\n",
      "             ('n_envs', 8),\n",
      "             ('n_steps', 8),\n",
      "             ('n_timesteps', 1000000.0),\n",
      "             ('normalize', True),\n",
      "             ('normalize_advantage', False),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(log_std_init=-2, ortho_init=False)'),\n",
      "             ('use_rms_prop', True),\n",
      "             ('use_sde', True),\n",
      "             ('vf_coef', 0.4)])\n",
      "Log path: logs/a2c/Pendulum-v1_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running a2c on MountainCarContinuous-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/a2c.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.0),\n",
      "             ('n_envs', 4),\n",
      "             ('n_steps', 100),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(log_std_init=0.0, ortho_init=False)'),\n",
      "             ('sde_sample_freq', 16),\n",
      "             ('use_sde', True)])\n",
      "Log path: logs/a2c/MountainCarContinuous-v0_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running ppo on CartPole-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ppo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 'lin_0.2'),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.8),\n",
      "             ('gamma', 0.98),\n",
      "             ('learning_rate', 'lin_0.001'),\n",
      "             ('n_envs', 8),\n",
      "             ('n_epochs', 20),\n",
      "             ('n_steps', 32),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/ppo/CartPole-v1_72\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running ppo on MountainCar-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ppo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.98),\n",
      "             ('gamma', 0.99),\n",
      "             ('n_envs', 16),\n",
      "             ('n_epochs', 4),\n",
      "             ('n_steps', 16),\n",
      "             ('n_timesteps', 1000000.0),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/ppo/MountainCar-v0_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running ppo on Acrobot-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ppo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.94),\n",
      "             ('gamma', 0.99),\n",
      "             ('n_envs', 16),\n",
      "             ('n_epochs', 4),\n",
      "             ('n_steps', 256),\n",
      "             ('n_timesteps', 1000000.0),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/ppo/Acrobot-v1_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running ppo on Pendulum-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ppo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('clip_range', 0.2),\n",
      "             ('ent_coef', 0.0),\n",
      "             ('gae_lambda', 0.95),\n",
      "             ('gamma', 0.9),\n",
      "             ('learning_rate', 0.001),\n",
      "             ('n_envs', 4),\n",
      "             ('n_epochs', 10),\n",
      "             ('n_steps', 1024),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('sde_sample_freq', 4),\n",
      "             ('use_sde', True)])\n",
      "Log path: logs/ppo/Pendulum-v1_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running ppo on MountainCarContinuous-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ppo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('clip_range', 0.1),\n",
      "             ('ent_coef', 0.00429),\n",
      "             ('gae_lambda', 0.9),\n",
      "             ('gamma', 0.9999),\n",
      "             ('learning_rate', 7.77e-05),\n",
      "             ('max_grad_norm', 5),\n",
      "             ('n_envs', 1),\n",
      "             ('n_epochs', 10),\n",
      "             ('n_steps', 8),\n",
      "             ('n_timesteps', 20000.0),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(log_std_init=-3.29, ortho_init=False)'),\n",
      "             ('use_sde', True),\n",
      "             ('vf_coef', 0.19)])\n",
      "Log path: logs/ppo/MountainCarContinuous-v0_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running dqn on CartPole-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/dqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 64),\n",
      "             ('buffer_size', 100000),\n",
      "             ('exploration_final_eps', 0.04),\n",
      "             ('exploration_fraction', 0.16),\n",
      "             ('gamma', 0.99),\n",
      "             ('gradient_steps', 128),\n",
      "             ('learning_rate', 0.0023),\n",
      "             ('learning_starts', 1000),\n",
      "             ('n_timesteps', 50000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[256, 256])'),\n",
      "             ('target_update_interval', 10),\n",
      "             ('train_freq', 256)])\n",
      "Log path: logs/dqn/CartPole-v1_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running dqn on MountainCar-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/dqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 128),\n",
      "             ('buffer_size', 10000),\n",
      "             ('exploration_final_eps', 0.07),\n",
      "             ('exploration_fraction', 0.2),\n",
      "             ('gamma', 0.98),\n",
      "             ('gradient_steps', 8),\n",
      "             ('learning_rate', 0.004),\n",
      "             ('learning_starts', 1000),\n",
      "             ('n_timesteps', 120000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[256, 256])'),\n",
      "             ('target_update_interval', 600),\n",
      "             ('train_freq', 16)])\n",
      "Log path: logs/dqn/MountainCar-v0_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running dqn on Acrobot-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/dqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 128),\n",
      "             ('buffer_size', 50000),\n",
      "             ('exploration_final_eps', 0.1),\n",
      "             ('exploration_fraction', 0.12),\n",
      "             ('gamma', 0.99),\n",
      "             ('gradient_steps', -1),\n",
      "             ('learning_rate', 0.00063),\n",
      "             ('learning_starts', 0),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[256, 256])'),\n",
      "             ('target_update_interval', 250),\n",
      "             ('train_freq', 4)])\n",
      "Log path: logs/dqn/Acrobot-v1_3\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running qrdqn on CartPole-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/qrdqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 64),\n",
      "             ('buffer_size', 100000),\n",
      "             ('exploration_final_eps', 0.04),\n",
      "             ('exploration_fraction', 0.16),\n",
      "             ('gamma', 0.99),\n",
      "             ('gradient_steps', 128),\n",
      "             ('learning_rate', 0.0023),\n",
      "             ('learning_starts', 1000),\n",
      "             ('n_timesteps', 50000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[256, 256], n_quantiles=10)'),\n",
      "             ('target_update_interval', 10),\n",
      "             ('train_freq', 256)])\n",
      "Log path: logs/qrdqn/CartPole-v1_1\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running qrdqn on MountainCar-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/qrdqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 128),\n",
      "             ('buffer_size', 10000),\n",
      "             ('exploration_final_eps', 0.07),\n",
      "             ('exploration_fraction', 0.2),\n",
      "             ('gamma', 0.98),\n",
      "             ('gradient_steps', 8),\n",
      "             ('learning_rate', 0.004),\n",
      "             ('learning_starts', 1000),\n",
      "             ('n_timesteps', 120000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[256, 256], n_quantiles=25)'),\n",
      "             ('target_update_interval', 600),\n",
      "             ('train_freq', 16)])\n",
      "Log path: logs/qrdqn/MountainCar-v0_1\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running qrdqn on Acrobot-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/qrdqn.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 128),\n",
      "             ('buffer_size', 50000),\n",
      "             ('exploration_final_eps', 0.1),\n",
      "             ('exploration_fraction', 0.12),\n",
      "             ('gamma', 0.99),\n",
      "             ('gradient_steps', -1),\n",
      "             ('learning_rate', 0.00063),\n",
      "             ('learning_starts', 0),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[256, 256], n_quantiles=25)'),\n",
      "             ('target_update_interval', 250),\n",
      "             ('train_freq', 4)])\n",
      "Log path: logs/qrdqn/Acrobot-v1_1\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running ddpg on Pendulum-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ddpg.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('buffer_size', 200000),\n",
      "             ('gamma', 0.98),\n",
      "             ('gradient_steps', 1),\n",
      "             ('learning_rate', 0.001),\n",
      "             ('learning_starts', 10000),\n",
      "             ('n_timesteps', 20000),\n",
      "             ('noise_std', 0.1),\n",
      "             ('noise_type', 'normal'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[400, 300])'),\n",
      "             ('train_freq', 1)])\n",
      "Applying normal noise with std 0.1\n",
      "Log path: logs/ddpg/Pendulum-v1_1\n",
      "policy:\n",
      "actor.optimizer:\n",
      "critic.optimizer:\n",
      "Running ddpg on MountainCarContinuous-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/ddpg.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('gradient_steps', 1),\n",
      "             ('learning_rate', 0.001),\n",
      "             ('n_timesteps', 300000),\n",
      "             ('noise_std', 0.5),\n",
      "             ('noise_type', 'ornstein-uhlenbeck'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[400, 300])'),\n",
      "             ('train_freq', 1)])\n",
      "Applying ornstein-uhlenbeck noise with std 0.5\n",
      "Log path: logs/ddpg/MountainCarContinuous-v0_1\n",
      "policy:\n",
      "actor.optimizer:\n",
      "critic.optimizer:\n",
      "Running sac on Pendulum-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/sac.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('learning_rate', 0.001),\n",
      "             ('n_timesteps', 20000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/sac/Pendulum-v1_1\n",
      "policy:\n",
      "actor.optimizer:\n",
      "critic.optimizer:\n",
      "ent_coef_optimizer:\n",
      "Running sac on MountainCarContinuous-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/sac.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 512),\n",
      "             ('buffer_size', 50000),\n",
      "             ('ent_coef', 0.1),\n",
      "             ('gamma', 0.9999),\n",
      "             ('gradient_steps', 32),\n",
      "             ('learning_rate', 0.0003),\n",
      "             ('learning_starts', 0),\n",
      "             ('n_timesteps', 50000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(log_std_init=-3.67, net_arch=[64, 64])'),\n",
      "             ('tau', 0.01),\n",
      "             ('train_freq', 32),\n",
      "             ('use_sde', True)])\n",
      "Log path: logs/sac/MountainCarContinuous-v0_1\n",
      "policy:\n",
      "actor.optimizer:\n",
      "critic.optimizer:\n",
      "Running td3 on Pendulum-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/td3.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('buffer_size', 200000),\n",
      "             ('gamma', 0.98),\n",
      "             ('gradient_steps', 1),\n",
      "             ('learning_rate', 0.001),\n",
      "             ('learning_starts', 10000),\n",
      "             ('n_timesteps', 20000),\n",
      "             ('noise_std', 0.1),\n",
      "             ('noise_type', 'normal'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[400, 300])'),\n",
      "             ('train_freq', 1)])\n",
      "Applying normal noise with std 0.1\n",
      "Log path: logs/td3/Pendulum-v1_1\n",
      "policy:\n",
      "actor.optimizer:\n",
      "critic.optimizer:\n",
      "Running td3 on MountainCarContinuous-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/td3.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 256),\n",
      "             ('gradient_steps', 1),\n",
      "             ('learning_rate', 0.001),\n",
      "             ('n_timesteps', 300000),\n",
      "             ('noise_std', 0.5),\n",
      "             ('noise_type', 'ornstein-uhlenbeck'),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(net_arch=[400, 300])'),\n",
      "             ('train_freq', 1)])\n",
      "Applying ornstein-uhlenbeck noise with std 0.5\n",
      "Log path: logs/td3/MountainCarContinuous-v0_1\n",
      "policy:\n",
      "actor.optimizer:\n",
      "critic.optimizer:\n",
      "Running tqc on Pendulum-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/tqc.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('learning_rate', 0.001),\n",
      "             ('n_timesteps', 20000),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/tqc/Pendulum-v1_1\n",
      "policy:\n",
      "actor.optimizer:\n",
      "critic.optimizer:\n",
      "ent_coef_optimizer:\n",
      "Running tqc on MountainCarContinuous-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/tqc.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 512),\n",
      "             ('buffer_size', 50000),\n",
      "             ('ent_coef', 0.1),\n",
      "             ('gamma', 0.9999),\n",
      "             ('gradient_steps', 32),\n",
      "             ('learning_rate', 0.0003),\n",
      "             ('learning_starts', 0),\n",
      "             ('n_timesteps', 50000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('policy_kwargs', 'dict(log_std_init=-3.67, net_arch=[64, 64])'),\n",
      "             ('tau', 0.01),\n",
      "             ('train_freq', 32),\n",
      "             ('use_sde', True)])\n",
      "Log path: logs/tqc/MountainCarContinuous-v0_1\n",
      "policy:\n",
      "actor.optimizer:\n",
      "critic.optimizer:\n",
      "Running trpo on CartPole-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/trpo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 512),\n",
      "             ('cg_damping', 0.001),\n",
      "             ('gae_lambda', 0.98),\n",
      "             ('gamma', 0.99),\n",
      "             ('learning_rate', 0.001),\n",
      "             ('n_critic_updates', 20),\n",
      "             ('n_envs', 2),\n",
      "             ('n_steps', 512),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/trpo/CartPole-v1_1\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running trpo on MountainCar-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/trpo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('n_critic_updates', 20),\n",
      "             ('n_envs', 2),\n",
      "             ('n_steps', 1024),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/trpo/MountainCar-v0_1\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running trpo on Acrobot-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/trpo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('n_envs', 2),\n",
      "             ('n_steps', 1024),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Log path: logs/trpo/Acrobot-v1_1\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running trpo on Pendulum-v1\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/trpo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('gamma', 0.9),\n",
      "             ('n_critic_updates', 15),\n",
      "             ('n_envs', 2),\n",
      "             ('n_steps', 1024),\n",
      "             ('n_timesteps', 100000.0),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('sde_sample_freq', 4),\n",
      "             ('use_sde', True)])\n",
      "Log path: logs/trpo/Pendulum-v1_1\n",
      "policy:\n",
      "policy.optimizer:\n",
      "Running trpo on MountainCarContinuous-v0\n",
      "Loading hyperparameters from: /home/startung/code/rl-baselines3-zoo-with-pruning/hyperparams/trpo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('n_envs', 2),\n",
      "             ('n_timesteps', 50000),\n",
      "             ('normalize', True),\n",
      "             ('policy', 'MlpPolicy'),\n",
      "             ('sde_sample_freq', 4),\n",
      "             ('use_sde', True)])\n",
      "Log path: logs/trpo/MountainCarContinuous-v0_1\n",
      "policy:\n",
      "policy.optimizer:\n"
     ]
    }
   ],
   "source": [
    "output_str = \"\"\n",
    "\n",
    "for key_name in algos:\n",
    "    for env_name in algos[key_name]:\n",
    "        print(f\"Running {key_name} on {env_name}\")\n",
    "        output_str += f\"Running {key_name} on {env_name}\\n\"\n",
    "        args = RlZooArgs(algo=key_name, env=env_name)\n",
    "\n",
    "        # Going through custom gym packages to let them register in the global registry\n",
    "        for env_module in args.gym_packages:\n",
    "            importlib.import_module(env_module)\n",
    "\n",
    "        env_id = args.env\n",
    "        registered_envs = set(gym.envs.registry.keys())\n",
    "\n",
    "        # If the environment is not found, suggest the closest match\n",
    "        if env_id not in registered_envs:\n",
    "            try:\n",
    "                closest_match = difflib.get_close_matches(env_id, registered_envs, n=1)[0]\n",
    "            except IndexError:\n",
    "                closest_match = \"'no close match found...'\"\n",
    "            raise ValueError(f\"{env_id} not found in gym registry, you maybe meant {closest_match}?\")\n",
    "\n",
    "        # Unique id to ensure there is no race condition for the folder creation\n",
    "        uuid_str = f\"_{uuid.uuid4()}\" if args.uuid else \"\"\n",
    "        if args.seed < 0:\n",
    "            # Seed but with a random one\n",
    "            args.seed = np.random.randint(2**32 - 1, dtype=\"int64\").item()  # type: ignore[attr-defined]\n",
    "\n",
    "        set_random_seed(args.seed)\n",
    "        # print(f\"Seed: {args.seed}\")\n",
    "\n",
    "        exp_manager = ExperimentManager(\n",
    "            args,\n",
    "            args.algo,\n",
    "            env_id,\n",
    "            args.log_folder,\n",
    "            args.tensorboard_log,\n",
    "            args.n_timesteps,\n",
    "            args.eval_freq,\n",
    "            args.eval_episodes,\n",
    "            args.save_freq,\n",
    "            args.hyperparams,\n",
    "            args.env_kwargs,\n",
    "            args.eval_env_kwargs,\n",
    "            args.trained_agent,\n",
    "            args.optimize_hyperparameters,\n",
    "            args.storage,\n",
    "            args.study_name,\n",
    "            args.n_trials,\n",
    "            args.max_total_trials,\n",
    "            args.n_jobs,\n",
    "            args.sampler,\n",
    "            args.pruner,\n",
    "            args.optimization_log_path,\n",
    "            n_startup_trials=args.n_startup_trials,\n",
    "            n_evaluations=args.n_evaluations,\n",
    "            truncate_last_trajectory=args.truncate_last_trajectory,\n",
    "            uuid_str=uuid_str,\n",
    "            seed=args.seed,\n",
    "            log_interval=args.log_interval,\n",
    "            save_replay_buffer=args.save_replay_buffer,\n",
    "            verbose=args.verbose,\n",
    "            vec_env_type=args.vec_env,\n",
    "            n_eval_envs=args.n_eval_envs,\n",
    "            no_optim_plots=args.no_optim_plots,\n",
    "            device=args.device,\n",
    "            config=args.conf_file,\n",
    "            show_progress=args.progress,\n",
    "        )\n",
    "\n",
    "        # Prepare experiment and launch hyperparameter optimization if needed\n",
    "        results = exp_manager.setup_experiment()\n",
    "        if results is not None:\n",
    "            model, saved_hyperparams = results\n",
    "\n",
    "            # Normal training\n",
    "            if model is not None:\n",
    "                for key, values in model.get_parameters().items():\n",
    "                    print(key, end=\":\\n\")\n",
    "                    output_str += f\"{key}:\\n\"\n",
    "                    for inner_key, value in values.items():\n",
    "                        # print(key, value.shape)\n",
    "                        if isinstance(value, th.Tensor):\n",
    "                            # print('\\t', inner_key, value.shape)\n",
    "                            output_str += f\"\\t {inner_key} {value.shape}\\n\"\n",
    "                        else:\n",
    "                            # print('\\t', inner_key, value)\n",
    "                            output_str += f\"\\t {inner_key} {value}\\n\\n-----------------------------\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ars on CartPole-v1\n",
      "policy:\n",
      "\t action_net.0.weight torch.Size([2, 4])\n",
      "Running ars on MountainCar-v0\n",
      "policy:\n",
      "\t action_net.0.weight torch.Size([16, 2])\n",
      "\t action_net.0.bias torch.Size([16])\n",
      "\t action_net.2.weight torch.Size([3, 16])\n",
      "\t action_net.2.bias torch.Size([3])\n",
      "Running ars on Acrobot-v1\n",
      "policy:\n",
      "\t action_net.0.weight torch.Size([16, 6])\n",
      "\t action_net.0.bias torch.Size([16])\n",
      "\t action_net.2.weight torch.Size([3, 16])\n",
      "\t action_net.2.bias torch.Size([3])\n",
      "Running ars on Pendulum-v1\n",
      "policy:\n",
      "\t action_net.0.weight torch.Size([16, 3])\n",
      "\t action_net.0.bias torch.Size([16])\n",
      "\t action_net.2.weight torch.Size([1, 16])\n",
      "\t action_net.2.bias torch.Size([1])\n",
      "Running ars on MountainCarContinuous-v0\n",
      "policy:\n",
      "\t action_net.0.weight torch.Size([16, 2])\n",
      "\t action_net.0.bias torch.Size([16])\n",
      "\t action_net.2.weight torch.Size([1, 16])\n",
      "\t action_net.2.bias torch.Size([1])\n",
      "Running a2c on CartPole-v1\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 4])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 4])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([2, 64])\n",
      "\t action_net.bias torch.Size([2])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0007, 'momentum': 0, 'alpha': 0.99, 'eps': 1e-05, 'centered': False, 'weight_decay': 0, 'capturable': False, 'foreach': None, 'maximize': False, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running a2c on MountainCar-v0\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([3, 64])\n",
      "\t action_net.bias torch.Size([3])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0007, 'momentum': 0, 'alpha': 0.99, 'eps': 1e-05, 'centered': False, 'weight_decay': 0, 'capturable': False, 'foreach': None, 'maximize': False, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running a2c on Acrobot-v1\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 6])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 6])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([3, 64])\n",
      "\t action_net.bias torch.Size([3])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0007, 'momentum': 0, 'alpha': 0.99, 'eps': 1e-05, 'centered': False, 'weight_decay': 0, 'capturable': False, 'foreach': None, 'maximize': False, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running a2c on Pendulum-v1\n",
      "policy:\n",
      "\t log_std torch.Size([64, 1])\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 3])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 3])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([1, 64])\n",
      "\t action_net.bias torch.Size([1])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0007, 'momentum': 0, 'alpha': 0.99, 'eps': 1e-05, 'centered': False, 'weight_decay': 0, 'capturable': False, 'foreach': None, 'maximize': False, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running a2c on MountainCarContinuous-v0\n",
      "policy:\n",
      "\t log_std torch.Size([64, 1])\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([1, 64])\n",
      "\t action_net.bias torch.Size([1])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0007, 'momentum': 0, 'alpha': 0.99, 'eps': 1e-05, 'centered': False, 'weight_decay': 0, 'capturable': False, 'foreach': None, 'maximize': False, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running ppo on CartPole-v1\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 4])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 4])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([2, 64])\n",
      "\t action_net.bias torch.Size([2])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running ppo on MountainCar-v0\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([3, 64])\n",
      "\t action_net.bias torch.Size([3])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running ppo on Acrobot-v1\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 6])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 6])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([3, 64])\n",
      "\t action_net.bias torch.Size([3])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running ppo on Pendulum-v1\n",
      "policy:\n",
      "\t log_std torch.Size([64, 1])\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 3])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 3])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([1, 64])\n",
      "\t action_net.bias torch.Size([1])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running ppo on MountainCarContinuous-v0\n",
      "policy:\n",
      "\t log_std torch.Size([64, 1])\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([1, 64])\n",
      "\t action_net.bias torch.Size([1])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 7.77e-05, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running dqn on CartPole-v1\n",
      "policy:\n",
      "\t q_net.q_net.0.weight torch.Size([256, 4])\n",
      "\t q_net.q_net.0.bias torch.Size([256])\n",
      "\t q_net.q_net.2.weight torch.Size([256, 256])\n",
      "\t q_net.q_net.2.bias torch.Size([256])\n",
      "\t q_net.q_net.4.weight torch.Size([2, 256])\n",
      "\t q_net.q_net.4.bias torch.Size([2])\n",
      "\t q_net_target.q_net.0.weight torch.Size([256, 4])\n",
      "\t q_net_target.q_net.0.bias torch.Size([256])\n",
      "\t q_net_target.q_net.2.weight torch.Size([256, 256])\n",
      "\t q_net_target.q_net.2.bias torch.Size([256])\n",
      "\t q_net_target.q_net.4.weight torch.Size([2, 256])\n",
      "\t q_net_target.q_net.4.bias torch.Size([2])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0023, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running dqn on MountainCar-v0\n",
      "policy:\n",
      "\t q_net.q_net.0.weight torch.Size([256, 2])\n",
      "\t q_net.q_net.0.bias torch.Size([256])\n",
      "\t q_net.q_net.2.weight torch.Size([256, 256])\n",
      "\t q_net.q_net.2.bias torch.Size([256])\n",
      "\t q_net.q_net.4.weight torch.Size([3, 256])\n",
      "\t q_net.q_net.4.bias torch.Size([3])\n",
      "\t q_net_target.q_net.0.weight torch.Size([256, 2])\n",
      "\t q_net_target.q_net.0.bias torch.Size([256])\n",
      "\t q_net_target.q_net.2.weight torch.Size([256, 256])\n",
      "\t q_net_target.q_net.2.bias torch.Size([256])\n",
      "\t q_net_target.q_net.4.weight torch.Size([3, 256])\n",
      "\t q_net_target.q_net.4.bias torch.Size([3])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.004, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running dqn on Acrobot-v1\n",
      "policy:\n",
      "\t q_net.q_net.0.weight torch.Size([256, 6])\n",
      "\t q_net.q_net.0.bias torch.Size([256])\n",
      "\t q_net.q_net.2.weight torch.Size([256, 256])\n",
      "\t q_net.q_net.2.bias torch.Size([256])\n",
      "\t q_net.q_net.4.weight torch.Size([3, 256])\n",
      "\t q_net.q_net.4.bias torch.Size([3])\n",
      "\t q_net_target.q_net.0.weight torch.Size([256, 6])\n",
      "\t q_net_target.q_net.0.bias torch.Size([256])\n",
      "\t q_net_target.q_net.2.weight torch.Size([256, 256])\n",
      "\t q_net_target.q_net.2.bias torch.Size([256])\n",
      "\t q_net_target.q_net.4.weight torch.Size([3, 256])\n",
      "\t q_net_target.q_net.4.bias torch.Size([3])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.00063, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running qrdqn on CartPole-v1\n",
      "policy:\n",
      "\t quantile_net.quantile_net.0.weight torch.Size([256, 4])\n",
      "\t quantile_net.quantile_net.0.bias torch.Size([256])\n",
      "\t quantile_net.quantile_net.2.weight torch.Size([256, 256])\n",
      "\t quantile_net.quantile_net.2.bias torch.Size([256])\n",
      "\t quantile_net.quantile_net.4.weight torch.Size([20, 256])\n",
      "\t quantile_net.quantile_net.4.bias torch.Size([20])\n",
      "\t quantile_net_target.quantile_net.0.weight torch.Size([256, 4])\n",
      "\t quantile_net_target.quantile_net.0.bias torch.Size([256])\n",
      "\t quantile_net_target.quantile_net.2.weight torch.Size([256, 256])\n",
      "\t quantile_net_target.quantile_net.2.bias torch.Size([256])\n",
      "\t quantile_net_target.quantile_net.4.weight torch.Size([20, 256])\n",
      "\t quantile_net_target.quantile_net.4.bias torch.Size([20])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0023, 'betas': (0.9, 0.999), 'eps': 0.00015625, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running qrdqn on MountainCar-v0\n",
      "policy:\n",
      "\t quantile_net.quantile_net.0.weight torch.Size([256, 2])\n",
      "\t quantile_net.quantile_net.0.bias torch.Size([256])\n",
      "\t quantile_net.quantile_net.2.weight torch.Size([256, 256])\n",
      "\t quantile_net.quantile_net.2.bias torch.Size([256])\n",
      "\t quantile_net.quantile_net.4.weight torch.Size([75, 256])\n",
      "\t quantile_net.quantile_net.4.bias torch.Size([75])\n",
      "\t quantile_net_target.quantile_net.0.weight torch.Size([256, 2])\n",
      "\t quantile_net_target.quantile_net.0.bias torch.Size([256])\n",
      "\t quantile_net_target.quantile_net.2.weight torch.Size([256, 256])\n",
      "\t quantile_net_target.quantile_net.2.bias torch.Size([256])\n",
      "\t quantile_net_target.quantile_net.4.weight torch.Size([75, 256])\n",
      "\t quantile_net_target.quantile_net.4.bias torch.Size([75])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.004, 'betas': (0.9, 0.999), 'eps': 7.8125e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running qrdqn on Acrobot-v1\n",
      "policy:\n",
      "\t quantile_net.quantile_net.0.weight torch.Size([256, 6])\n",
      "\t quantile_net.quantile_net.0.bias torch.Size([256])\n",
      "\t quantile_net.quantile_net.2.weight torch.Size([256, 256])\n",
      "\t quantile_net.quantile_net.2.bias torch.Size([256])\n",
      "\t quantile_net.quantile_net.4.weight torch.Size([75, 256])\n",
      "\t quantile_net.quantile_net.4.bias torch.Size([75])\n",
      "\t quantile_net_target.quantile_net.0.weight torch.Size([256, 6])\n",
      "\t quantile_net_target.quantile_net.0.bias torch.Size([256])\n",
      "\t quantile_net_target.quantile_net.2.weight torch.Size([256, 256])\n",
      "\t quantile_net_target.quantile_net.2.bias torch.Size([256])\n",
      "\t quantile_net_target.quantile_net.4.weight torch.Size([75, 256])\n",
      "\t quantile_net_target.quantile_net.4.bias torch.Size([75])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.00063, 'betas': (0.9, 0.999), 'eps': 7.8125e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running ddpg on Pendulum-v1\n",
      "policy:\n",
      "\t actor.mu.0.weight torch.Size([400, 3])\n",
      "\t actor.mu.0.bias torch.Size([400])\n",
      "\t actor.mu.2.weight torch.Size([300, 400])\n",
      "\t actor.mu.2.bias torch.Size([300])\n",
      "\t actor.mu.4.weight torch.Size([1, 300])\n",
      "\t actor.mu.4.bias torch.Size([1])\n",
      "\t actor_target.mu.0.weight torch.Size([400, 3])\n",
      "\t actor_target.mu.0.bias torch.Size([400])\n",
      "\t actor_target.mu.2.weight torch.Size([300, 400])\n",
      "\t actor_target.mu.2.bias torch.Size([300])\n",
      "\t actor_target.mu.4.weight torch.Size([1, 300])\n",
      "\t actor_target.mu.4.bias torch.Size([1])\n",
      "\t critic.qf0.0.weight torch.Size([400, 4])\n",
      "\t critic.qf0.0.bias torch.Size([400])\n",
      "\t critic.qf0.2.weight torch.Size([300, 400])\n",
      "\t critic.qf0.2.bias torch.Size([300])\n",
      "\t critic.qf0.4.weight torch.Size([1, 300])\n",
      "\t critic.qf0.4.bias torch.Size([1])\n",
      "\t critic_target.qf0.0.weight torch.Size([400, 4])\n",
      "\t critic_target.qf0.0.bias torch.Size([400])\n",
      "\t critic_target.qf0.2.weight torch.Size([300, 400])\n",
      "\t critic_target.qf0.2.bias torch.Size([300])\n",
      "\t critic_target.qf0.4.weight torch.Size([1, 300])\n",
      "\t critic_target.qf0.4.bias torch.Size([1])\n",
      "actor.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "critic.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running ddpg on MountainCarContinuous-v0\n",
      "policy:\n",
      "\t actor.mu.0.weight torch.Size([400, 2])\n",
      "\t actor.mu.0.bias torch.Size([400])\n",
      "\t actor.mu.2.weight torch.Size([300, 400])\n",
      "\t actor.mu.2.bias torch.Size([300])\n",
      "\t actor.mu.4.weight torch.Size([1, 300])\n",
      "\t actor.mu.4.bias torch.Size([1])\n",
      "\t actor_target.mu.0.weight torch.Size([400, 2])\n",
      "\t actor_target.mu.0.bias torch.Size([400])\n",
      "\t actor_target.mu.2.weight torch.Size([300, 400])\n",
      "\t actor_target.mu.2.bias torch.Size([300])\n",
      "\t actor_target.mu.4.weight torch.Size([1, 300])\n",
      "\t actor_target.mu.4.bias torch.Size([1])\n",
      "\t critic.qf0.0.weight torch.Size([400, 3])\n",
      "\t critic.qf0.0.bias torch.Size([400])\n",
      "\t critic.qf0.2.weight torch.Size([300, 400])\n",
      "\t critic.qf0.2.bias torch.Size([300])\n",
      "\t critic.qf0.4.weight torch.Size([1, 300])\n",
      "\t critic.qf0.4.bias torch.Size([1])\n",
      "\t critic_target.qf0.0.weight torch.Size([400, 3])\n",
      "\t critic_target.qf0.0.bias torch.Size([400])\n",
      "\t critic_target.qf0.2.weight torch.Size([300, 400])\n",
      "\t critic_target.qf0.2.bias torch.Size([300])\n",
      "\t critic_target.qf0.4.weight torch.Size([1, 300])\n",
      "\t critic_target.qf0.4.bias torch.Size([1])\n",
      "actor.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "critic.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running sac on Pendulum-v1\n",
      "policy:\n",
      "\t actor.latent_pi.0.weight torch.Size([256, 3])\n",
      "\t actor.latent_pi.0.bias torch.Size([256])\n",
      "\t actor.latent_pi.2.weight torch.Size([256, 256])\n",
      "\t actor.latent_pi.2.bias torch.Size([256])\n",
      "\t actor.mu.weight torch.Size([1, 256])\n",
      "\t actor.mu.bias torch.Size([1])\n",
      "\t actor.log_std.weight torch.Size([1, 256])\n",
      "\t actor.log_std.bias torch.Size([1])\n",
      "\t critic.qf0.0.weight torch.Size([256, 4])\n",
      "\t critic.qf0.0.bias torch.Size([256])\n",
      "\t critic.qf0.2.weight torch.Size([256, 256])\n",
      "\t critic.qf0.2.bias torch.Size([256])\n",
      "\t critic.qf0.4.weight torch.Size([1, 256])\n",
      "\t critic.qf0.4.bias torch.Size([1])\n",
      "\t critic.qf1.0.weight torch.Size([256, 4])\n",
      "\t critic.qf1.0.bias torch.Size([256])\n",
      "\t critic.qf1.2.weight torch.Size([256, 256])\n",
      "\t critic.qf1.2.bias torch.Size([256])\n",
      "\t critic.qf1.4.weight torch.Size([1, 256])\n",
      "\t critic.qf1.4.bias torch.Size([1])\n",
      "\t critic_target.qf0.0.weight torch.Size([256, 4])\n",
      "\t critic_target.qf0.0.bias torch.Size([256])\n",
      "\t critic_target.qf0.2.weight torch.Size([256, 256])\n",
      "\t critic_target.qf0.2.bias torch.Size([256])\n",
      "\t critic_target.qf0.4.weight torch.Size([1, 256])\n",
      "\t critic_target.qf0.4.bias torch.Size([1])\n",
      "\t critic_target.qf1.0.weight torch.Size([256, 4])\n",
      "\t critic_target.qf1.0.bias torch.Size([256])\n",
      "\t critic_target.qf1.2.weight torch.Size([256, 256])\n",
      "\t critic_target.qf1.2.bias torch.Size([256])\n",
      "\t critic_target.qf1.4.weight torch.Size([1, 256])\n",
      "\t critic_target.qf1.4.bias torch.Size([1])\n",
      "actor.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "critic.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "ent_coef_optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running sac on MountainCarContinuous-v0\n",
      "policy:\n",
      "\t actor.log_std torch.Size([64, 1])\n",
      "\t actor.latent_pi.0.weight torch.Size([64, 2])\n",
      "\t actor.latent_pi.0.bias torch.Size([64])\n",
      "\t actor.latent_pi.2.weight torch.Size([64, 64])\n",
      "\t actor.latent_pi.2.bias torch.Size([64])\n",
      "\t actor.mu.0.weight torch.Size([1, 64])\n",
      "\t actor.mu.0.bias torch.Size([1])\n",
      "\t critic.qf0.0.weight torch.Size([64, 3])\n",
      "\t critic.qf0.0.bias torch.Size([64])\n",
      "\t critic.qf0.2.weight torch.Size([64, 64])\n",
      "\t critic.qf0.2.bias torch.Size([64])\n",
      "\t critic.qf0.4.weight torch.Size([1, 64])\n",
      "\t critic.qf0.4.bias torch.Size([1])\n",
      "\t critic.qf1.0.weight torch.Size([64, 3])\n",
      "\t critic.qf1.0.bias torch.Size([64])\n",
      "\t critic.qf1.2.weight torch.Size([64, 64])\n",
      "\t critic.qf1.2.bias torch.Size([64])\n",
      "\t critic.qf1.4.weight torch.Size([1, 64])\n",
      "\t critic.qf1.4.bias torch.Size([1])\n",
      "\t critic_target.qf0.0.weight torch.Size([64, 3])\n",
      "\t critic_target.qf0.0.bias torch.Size([64])\n",
      "\t critic_target.qf0.2.weight torch.Size([64, 64])\n",
      "\t critic_target.qf0.2.bias torch.Size([64])\n",
      "\t critic_target.qf0.4.weight torch.Size([1, 64])\n",
      "\t critic_target.qf0.4.bias torch.Size([1])\n",
      "\t critic_target.qf1.0.weight torch.Size([64, 3])\n",
      "\t critic_target.qf1.0.bias torch.Size([64])\n",
      "\t critic_target.qf1.2.weight torch.Size([64, 64])\n",
      "\t critic_target.qf1.2.bias torch.Size([64])\n",
      "\t critic_target.qf1.4.weight torch.Size([1, 64])\n",
      "\t critic_target.qf1.4.bias torch.Size([1])\n",
      "actor.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "critic.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running td3 on Pendulum-v1\n",
      "policy:\n",
      "\t actor.mu.0.weight torch.Size([400, 3])\n",
      "\t actor.mu.0.bias torch.Size([400])\n",
      "\t actor.mu.2.weight torch.Size([300, 400])\n",
      "\t actor.mu.2.bias torch.Size([300])\n",
      "\t actor.mu.4.weight torch.Size([1, 300])\n",
      "\t actor.mu.4.bias torch.Size([1])\n",
      "\t actor_target.mu.0.weight torch.Size([400, 3])\n",
      "\t actor_target.mu.0.bias torch.Size([400])\n",
      "\t actor_target.mu.2.weight torch.Size([300, 400])\n",
      "\t actor_target.mu.2.bias torch.Size([300])\n",
      "\t actor_target.mu.4.weight torch.Size([1, 300])\n",
      "\t actor_target.mu.4.bias torch.Size([1])\n",
      "\t critic.qf0.0.weight torch.Size([400, 4])\n",
      "\t critic.qf0.0.bias torch.Size([400])\n",
      "\t critic.qf0.2.weight torch.Size([300, 400])\n",
      "\t critic.qf0.2.bias torch.Size([300])\n",
      "\t critic.qf0.4.weight torch.Size([1, 300])\n",
      "\t critic.qf0.4.bias torch.Size([1])\n",
      "\t critic.qf1.0.weight torch.Size([400, 4])\n",
      "\t critic.qf1.0.bias torch.Size([400])\n",
      "\t critic.qf1.2.weight torch.Size([300, 400])\n",
      "\t critic.qf1.2.bias torch.Size([300])\n",
      "\t critic.qf1.4.weight torch.Size([1, 300])\n",
      "\t critic.qf1.4.bias torch.Size([1])\n",
      "\t critic_target.qf0.0.weight torch.Size([400, 4])\n",
      "\t critic_target.qf0.0.bias torch.Size([400])\n",
      "\t critic_target.qf0.2.weight torch.Size([300, 400])\n",
      "\t critic_target.qf0.2.bias torch.Size([300])\n",
      "\t critic_target.qf0.4.weight torch.Size([1, 300])\n",
      "\t critic_target.qf0.4.bias torch.Size([1])\n",
      "\t critic_target.qf1.0.weight torch.Size([400, 4])\n",
      "\t critic_target.qf1.0.bias torch.Size([400])\n",
      "\t critic_target.qf1.2.weight torch.Size([300, 400])\n",
      "\t critic_target.qf1.2.bias torch.Size([300])\n",
      "\t critic_target.qf1.4.weight torch.Size([1, 300])\n",
      "\t critic_target.qf1.4.bias torch.Size([1])\n",
      "actor.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "critic.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running td3 on MountainCarContinuous-v0\n",
      "policy:\n",
      "\t actor.mu.0.weight torch.Size([400, 2])\n",
      "\t actor.mu.0.bias torch.Size([400])\n",
      "\t actor.mu.2.weight torch.Size([300, 400])\n",
      "\t actor.mu.2.bias torch.Size([300])\n",
      "\t actor.mu.4.weight torch.Size([1, 300])\n",
      "\t actor.mu.4.bias torch.Size([1])\n",
      "\t actor_target.mu.0.weight torch.Size([400, 2])\n",
      "\t actor_target.mu.0.bias torch.Size([400])\n",
      "\t actor_target.mu.2.weight torch.Size([300, 400])\n",
      "\t actor_target.mu.2.bias torch.Size([300])\n",
      "\t actor_target.mu.4.weight torch.Size([1, 300])\n",
      "\t actor_target.mu.4.bias torch.Size([1])\n",
      "\t critic.qf0.0.weight torch.Size([400, 3])\n",
      "\t critic.qf0.0.bias torch.Size([400])\n",
      "\t critic.qf0.2.weight torch.Size([300, 400])\n",
      "\t critic.qf0.2.bias torch.Size([300])\n",
      "\t critic.qf0.4.weight torch.Size([1, 300])\n",
      "\t critic.qf0.4.bias torch.Size([1])\n",
      "\t critic.qf1.0.weight torch.Size([400, 3])\n",
      "\t critic.qf1.0.bias torch.Size([400])\n",
      "\t critic.qf1.2.weight torch.Size([300, 400])\n",
      "\t critic.qf1.2.bias torch.Size([300])\n",
      "\t critic.qf1.4.weight torch.Size([1, 300])\n",
      "\t critic.qf1.4.bias torch.Size([1])\n",
      "\t critic_target.qf0.0.weight torch.Size([400, 3])\n",
      "\t critic_target.qf0.0.bias torch.Size([400])\n",
      "\t critic_target.qf0.2.weight torch.Size([300, 400])\n",
      "\t critic_target.qf0.2.bias torch.Size([300])\n",
      "\t critic_target.qf0.4.weight torch.Size([1, 300])\n",
      "\t critic_target.qf0.4.bias torch.Size([1])\n",
      "\t critic_target.qf1.0.weight torch.Size([400, 3])\n",
      "\t critic_target.qf1.0.bias torch.Size([400])\n",
      "\t critic_target.qf1.2.weight torch.Size([300, 400])\n",
      "\t critic_target.qf1.2.bias torch.Size([300])\n",
      "\t critic_target.qf1.4.weight torch.Size([1, 300])\n",
      "\t critic_target.qf1.4.bias torch.Size([1])\n",
      "actor.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "critic.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running tqc on Pendulum-v1\n",
      "policy:\n",
      "\t actor.latent_pi.0.weight torch.Size([256, 3])\n",
      "\t actor.latent_pi.0.bias torch.Size([256])\n",
      "\t actor.latent_pi.2.weight torch.Size([256, 256])\n",
      "\t actor.latent_pi.2.bias torch.Size([256])\n",
      "\t actor.mu.weight torch.Size([1, 256])\n",
      "\t actor.mu.bias torch.Size([1])\n",
      "\t actor.log_std.weight torch.Size([1, 256])\n",
      "\t actor.log_std.bias torch.Size([1])\n",
      "\t critic.qf0.0.weight torch.Size([256, 4])\n",
      "\t critic.qf0.0.bias torch.Size([256])\n",
      "\t critic.qf0.2.weight torch.Size([256, 256])\n",
      "\t critic.qf0.2.bias torch.Size([256])\n",
      "\t critic.qf0.4.weight torch.Size([25, 256])\n",
      "\t critic.qf0.4.bias torch.Size([25])\n",
      "\t critic.qf1.0.weight torch.Size([256, 4])\n",
      "\t critic.qf1.0.bias torch.Size([256])\n",
      "\t critic.qf1.2.weight torch.Size([256, 256])\n",
      "\t critic.qf1.2.bias torch.Size([256])\n",
      "\t critic.qf1.4.weight torch.Size([25, 256])\n",
      "\t critic.qf1.4.bias torch.Size([25])\n",
      "\t critic_target.qf0.0.weight torch.Size([256, 4])\n",
      "\t critic_target.qf0.0.bias torch.Size([256])\n",
      "\t critic_target.qf0.2.weight torch.Size([256, 256])\n",
      "\t critic_target.qf0.2.bias torch.Size([256])\n",
      "\t critic_target.qf0.4.weight torch.Size([25, 256])\n",
      "\t critic_target.qf0.4.bias torch.Size([25])\n",
      "\t critic_target.qf1.0.weight torch.Size([256, 4])\n",
      "\t critic_target.qf1.0.bias torch.Size([256])\n",
      "\t critic_target.qf1.2.weight torch.Size([256, 256])\n",
      "\t critic_target.qf1.2.bias torch.Size([256])\n",
      "\t critic_target.qf1.4.weight torch.Size([25, 256])\n",
      "\t critic_target.qf1.4.bias torch.Size([25])\n",
      "actor.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "critic.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "ent_coef_optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running tqc on MountainCarContinuous-v0\n",
      "policy:\n",
      "\t actor.log_std torch.Size([64, 1])\n",
      "\t actor.latent_pi.0.weight torch.Size([64, 2])\n",
      "\t actor.latent_pi.0.bias torch.Size([64])\n",
      "\t actor.latent_pi.2.weight torch.Size([64, 64])\n",
      "\t actor.latent_pi.2.bias torch.Size([64])\n",
      "\t actor.mu.0.weight torch.Size([1, 64])\n",
      "\t actor.mu.0.bias torch.Size([1])\n",
      "\t critic.qf0.0.weight torch.Size([64, 3])\n",
      "\t critic.qf0.0.bias torch.Size([64])\n",
      "\t critic.qf0.2.weight torch.Size([64, 64])\n",
      "\t critic.qf0.2.bias torch.Size([64])\n",
      "\t critic.qf0.4.weight torch.Size([25, 64])\n",
      "\t critic.qf0.4.bias torch.Size([25])\n",
      "\t critic.qf1.0.weight torch.Size([64, 3])\n",
      "\t critic.qf1.0.bias torch.Size([64])\n",
      "\t critic.qf1.2.weight torch.Size([64, 64])\n",
      "\t critic.qf1.2.bias torch.Size([64])\n",
      "\t critic.qf1.4.weight torch.Size([25, 64])\n",
      "\t critic.qf1.4.bias torch.Size([25])\n",
      "\t critic_target.qf0.0.weight torch.Size([64, 3])\n",
      "\t critic_target.qf0.0.bias torch.Size([64])\n",
      "\t critic_target.qf0.2.weight torch.Size([64, 64])\n",
      "\t critic_target.qf0.2.bias torch.Size([64])\n",
      "\t critic_target.qf0.4.weight torch.Size([25, 64])\n",
      "\t critic_target.qf0.4.bias torch.Size([25])\n",
      "\t critic_target.qf1.0.weight torch.Size([64, 3])\n",
      "\t critic_target.qf1.0.bias torch.Size([64])\n",
      "\t critic_target.qf1.2.weight torch.Size([64, 64])\n",
      "\t critic_target.qf1.2.bias torch.Size([64])\n",
      "\t critic_target.qf1.4.weight torch.Size([25, 64])\n",
      "\t critic_target.qf1.4.bias torch.Size([25])\n",
      "actor.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "critic.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running trpo on CartPole-v1\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 4])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 4])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([2, 64])\n",
      "\t action_net.bias torch.Size([2])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running trpo on MountainCar-v0\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([3, 64])\n",
      "\t action_net.bias torch.Size([3])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running trpo on Acrobot-v1\n",
      "policy:\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 6])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 6])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([3, 64])\n",
      "\t action_net.bias torch.Size([3])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running trpo on Pendulum-v1\n",
      "policy:\n",
      "\t log_std torch.Size([64, 1])\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 3])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 3])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([1, 64])\n",
      "\t action_net.bias torch.Size([1])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Running trpo on MountainCarContinuous-v0\n",
      "policy:\n",
      "\t log_std torch.Size([64, 1])\n",
      "\t mlp_extractor.policy_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.policy_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.policy_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.policy_net.2.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.0.weight torch.Size([64, 2])\n",
      "\t mlp_extractor.value_net.0.bias torch.Size([64])\n",
      "\t mlp_extractor.value_net.2.weight torch.Size([64, 64])\n",
      "\t mlp_extractor.value_net.2.bias torch.Size([64])\n",
      "\t action_net.weight torch.Size([1, 64])\n",
      "\t action_net.bias torch.Size([1])\n",
      "\t value_net.weight torch.Size([1, 64])\n",
      "\t value_net.bias torch.Size([1])\n",
      "policy.optimizer:\n",
      "\t state {}\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\t param_groups [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}]\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
